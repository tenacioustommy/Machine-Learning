{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self,input_channels,num_channels,use_1x1conv=False,strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(input_channels,num_channels,kernel_size=3,padding=1,stride=strides)\n",
    "        self.conv2=nn.Conv2d(num_channels,num_channels,kernel_size=3,padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3=nn.Conv2d(input_channels,num_channels,kernel_size=1,stride=strides)\n",
    "        else:\n",
    "            self.conv3=None\n",
    "        self.bn1=nn.BatchNorm2d(num_channels)\n",
    "        self.bn2=nn.BatchNorm2d(num_channels)\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "    def forward(self,X):\n",
    "        Y=F.relu(self.bn1(self.conv1(X)))\n",
    "        Y=self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X=self.conv3(X)\n",
    "        Y+=X\n",
    "        return F.relu(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(input_channels, num_channels, num_residuals,first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channels,num_channels, use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels,num_channels))\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "b1= nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2=block(64,64,2,first_block=True)\n",
    "b3=block(64,128,2)\n",
    "b4=block(128,256,2)\n",
    "b5=block(256,512,2)\n",
    "net=nn.Sequential(b1,b2,b3,b4,b5,nn.AdaptiveAvgPool2d((1,1)),nn.Flatten(),nn.Linear(512,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:  torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:  torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:  torch.Size([1, 128, 28, 28])\n",
      "Sequential output shape:  torch.Size([1, 256, 14, 14])\n",
      "Sequential output shape:  torch.Size([1, 512, 7, 7])\n",
      "AdaptiveAvgPool2d output shape:  torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape:  torch.Size([1, 512])\n",
      "Linear output shape:  torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "utils.shape(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "training on cuda:0\n",
      "loss 1.966, train acc 0.315, test acc 0.405\n",
      "loss 1.413, train acc 0.488, test acc 0.446\n",
      "loss 1.225, train acc 0.563, test acc 0.582\n",
      "loss 1.090, train acc 0.612, test acc 0.542\n",
      "loss 0.984, train acc 0.653, test acc 0.629\n",
      "loss 0.910, train acc 0.678, test acc 0.604\n",
      "loss 0.832, train acc 0.707, test acc 0.655\n",
      "loss 0.772, train acc 0.729, test acc 0.641\n",
      "loss 0.707, train acc 0.752, test acc 0.659\n",
      "loss 0.658, train acc 0.770, test acc 0.712\n",
      "2721.0 examples/sec on cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr,epochs,batch_size=0.05,10,128\n",
    "train_iter,test_iter=utils.load_data_cifar10(batch_size,resize=64,transform=True)\n",
    "utils.train_ch6(net,train_iter,test_iter,epochs,lr,utils.try_gpu(),0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
