{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import utils\n",
    "import d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size,num_hiddens,device):\n",
    "    num_inputs=num_outputs=vocab_size\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape,device=device)*0.01\n",
    "    W_xh=normal((num_inputs,num_hiddens))\n",
    "    W_hh=normal((num_hiddens,num_hiddens))\n",
    "    b_h=torch.zeros(num_hiddens,device=device)\n",
    "    W_hq=normal((num_hiddens,num_outputs))\n",
    "    b_q=torch.zeros(num_outputs,device=device)\n",
    "    params=[W_xh,W_hh,b_h,W_hq,b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size,num_hiddens,device):\n",
    "        return (torch.zeros((batch_size,num_hiddens),device=device),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs,state,params):\n",
    "    W_xh,W_hh,b_h,W_hq,b_q=params\n",
    "    H,=state\n",
    "    outputs=[]\n",
    "    for X in inputs:\n",
    "        H=torch.tanh(torch.mm(X,W_xh)+\n",
    "                     torch.mm(H,W_hh)+\n",
    "                     b_h)\n",
    "        Y=torch.mm(H,W_hq)+b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs,dim=0),(H,)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self,vocab_size,num_hiddens,device,get_params,init_state,forward_fn) -> None:\n",
    "        self.vocab_size,self.num_hiddens=vocab_size,num_hiddens\n",
    "        self.params=get_params(vocab_size,num_hiddens,device)\n",
    "        self.init_state,self.forward_fn=init_state,forward_fn\n",
    "    def __call__(self,X,state):\n",
    "        X=F.one_hot(X.T,self.vocab_size).type(torch.float32)\n",
    "        return self.forward_fn(X,state,self.params)\n",
    "    def begin_state(self,batch_size,device):\n",
    "        return self.init_state(batch_size,self.num_hiddens,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicts(prefix,num_preds,net,vocab,device):\n",
    "    state=net.begin_state(batch_size=1,device=device)\n",
    "    outputs=[vocab[prefix[0]]]\n",
    "    get_input=lambda:torch.tensor([outputs[-1]],device=device).reshape(1,1)\n",
    "    for y in prefix[1:]:\n",
    "        _,state=net(get_input(),state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):\n",
    "        y,state=net(get_input(),state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs ])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net,theta):\n",
    "    if isinstance(net,nn.Module):\n",
    "        params=[p for p in net.parameters()if p.requires_grad]\n",
    "    else:\n",
    "        params=net.params\n",
    "    norm=torch.sqrt(sum(torch.sum((p.grad**2))for p in params))\n",
    "    if norm>theta:\n",
    "        for param in params:\n",
    "            param.grad[:]*=theta/norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainepoch(net,train_iter,loss,updater,device,use_random_iter):\n",
    "    #use_random_iter 下一个batch跟上一个batch的第i个样本有没有关系\n",
    "\n",
    "    state,timer=None,utils.Timer()\n",
    "    metric=utils.Accumulator(2)\n",
    "    for X,Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            state=net.begin_state(batch_size=X.shape[0],device=device)\n",
    "        else:\n",
    "            if isinstance(net,nn.Module)and not isinstance(state):\n",
    "                state.detach_()\n",
    "            else:\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y=Y.T.reshape(-1)\n",
    "        X,y=X.to(device),y.to(device)\n",
    "        y_hat,state=net(X,state)\n",
    "        l=loss(y_hat,y.long()).mean()\n",
    "        if isinstance(updater,torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net,1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net,1)\n",
    "            updater(batch_size=1)\n",
    "        metric.add(l*y.numel(),y.numel())\n",
    "    return math.exp(metric[0]/metric[1]),metric[1]/timer.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,train_iter,vocab,lr,num_epochs,device,use_random_iter=False):\n",
    "    loss=nn.CrossEntropyLoss()\n",
    "    updater=torch.optim.SGD(net.parameters(),lr)\n",
    "    predict=lambda prefix:predicts(prefix,50,net,vocab,device)\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl,speed=trainepoch(net,train_iter,loss,updater,device,use_random_iter)\n",
    "        if (epoch+1)%10==0:\n",
    "            print(predict('time traveller'))\n",
    "    print(f'{ppl:.1f},{speed:.1f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12984\\84268477.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrnn_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_hiddens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_hiddens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "num_hiddens=256\n",
    "batch_size=32\n",
    "vocab=1\n",
    "rnn_layer=nn.RNN(len(vocab),num_hiddens)\n",
    "state=torch.zeros((1,batch_size,num_hiddens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.rand(size=())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
